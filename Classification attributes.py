# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DqePrWEHxCxGmGI-fohiapFNERxNvGPu

SS991
"""

from ucimlrepo import fetch_ucirepo

# fetch dataset
covertype = fetch_ucirepo(id=31)

# data (as pandas dataframes)
X = covertype.data.features
y = covertype.data.targets

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import fetch_openml

# Load the Forest Cover Type dataset
covertype = fetch_openml(name="covertype", version=4)
X = covertype.data
y = covertype.target.astype('int')

"""**How does the dataset look like?**"""

# Display the first few rows of the dataset
print("\nFirst few rows of the dataset:")
X.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features (important for SGD)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""How do we preprocess the data for SGDClassifier?"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features (important for SGD)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""How do we initialize and train the SGDClassifier?"""

# Initialize and train the SGDClassifier
sgd_classifier = SGDClassifier(random_state=42)
sgd_classifier.fit(X_train_scaled, y_train)

"""How do we make predictions and evaluate the model?"""

# Make predictions on the test set
y_pred = sgd_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Print evaluation results
print("\nModel Evaluation:")
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)

"""Display variable information"""

# Display variable information
print("\nVariable Information:")
print(covertype.feature_names)

"""**What are the summary statistics of the dataset?**"""

# Display summary statistics of the dataset
print("\nSummary Statistics:")
X.describe()

"""**How are the target labels distributed?**"""

# Display the distribution of target labels
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.countplot(x=y)
plt.title("Distribution of Target Labels")
plt.show()

"""**Can we visualize the relationships between some key features?**"""

# Visualize relationships between features (e.g., Elevation, Slope)
sns.pairplot(X[['Elevation', 'Slope', 'Horizontal_Distance_To_Roadways']])
plt.show()

"""**How sensitive is the model to different hyperparameter values?**"""

# Experiment with different learning rates and penalties
learning_rates = [0.001, 0.01, 0.1]
penalties = ['l1', 'l2']

for lr in learning_rates:
    for penalty in penalties:
        sgd_classifier = SGDClassifier(learning_rate='constant', eta0=lr, penalty=penalty, random_state=42)
        sgd_classifier.fit(X_train_scaled, y_train)
        y_pred = sgd_classifier.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"Learning Rate: {lr}, Penalty: {penalty}, Accuracy: {accuracy:.4f}")

"""**Are there any correlations between features?**"""

# Display a correlation matrix
correlation_matrix = X.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""**What are the most important features according to the trained model?**"""

# Display feature importance
feature_importance = sgd_classifier.coef_[0]
feature_names = X.columns

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(12, 12))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title("Feature Importance")
plt.show()

"""**How does the model perform with different batch sizes during training?**"""

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

# Experiment with different batch sizes
batch_sizes = [32, 64, 128]

for batch_size in batch_sizes:
    sgd_classifier_batch = SGDClassifier(random_state=42, max_iter=1000)

    # Use partial_fit to fit the model with the specified batch size
    for i in range(0, len(X_train_scaled), batch_size):
        X_batch = X_train_scaled[i:i + batch_size]
        y_batch = y_train[i:i + batch_size]
        sgd_classifier_batch.partial_fit(X_batch, y_batch, classes=np.unique(y_train))

    y_pred_batch = sgd_classifier_batch.predict(X_test_scaled)
    accuracy_batch = accuracy_score(y_test, y_pred_batch)
    print(f"Batch Size: {batch_size}, Accuracy: {accuracy_batch:.4f}")

"""**How does the model performance change with different numbers of iterations?**"""

# Experiment with different numbers of iterations
iterations = [100, 500, 1000]

for iteration in iterations:
    sgd_classifier_iter = SGDClassifier(random_state=42, max_iter=iteration)
    sgd_classifier_iter.fit(X_train_scaled, y_train)
    y_pred_iter = sgd_classifier_iter.predict(X_test_scaled)
    accuracy_iter = accuracy_score(y_test, y_pred_iter)
    print(f"Iterations: {iteration}, Accuracy: {accuracy_iter:.4f}")

"""**How does the model perform with different learning rates?**"""

# Experiment with different learning rates
learning_rates = [0.001, 0.01, 0.1]

for lr in learning_rates:
    sgd_classifier_lr = SGDClassifier(random_state=42, max_iter=1000, learning_rate='constant', eta0=lr)
    sgd_classifier_lr.fit(X_train_scaled, y_train)
    y_pred_lr = sgd_classifier_lr.predict(X_test_scaled)
    accuracy_lr = accuracy_score(y_test, y_pred_lr)
    print(f"Learning Rate: {lr}, Accuracy: {accuracy_lr:.4f}")

"""**How does the model perform with different regularization penalties?**"""

# Experiment with different regularization penalties
penalties = ['l1', 'l2']

for penalty in penalties:
    sgd_classifier_penalty = SGDClassifier(random_state=42, max_iter=1000, penalty=penalty)
    sgd_classifier_penalty.fit(X_train_scaled, y_train)
    y_pred_penalty = sgd_classifier_penalty.predict(X_test_scaled)
    accuracy_penalty = accuracy_score(y_test, y_pred_penalty)
    print(f"Penalty: {penalty}, Accuracy: {accuracy_penalty:.4f}")

"""**How does the model perform with different alpha values for regularization?**"""

# Experiment with different alpha values (regularization strength)
alphas = [0.0001, 0.001, 0.01]

for alpha in alphas:
    sgd_classifier_alpha = SGDClassifier(random_state=42, max_iter=1000, alpha=alpha)
    sgd_classifier_alpha.fit(X_train_scaled, y_train)
    y_pred_alpha = sgd_classifier_alpha.predict(X_test_scaled)
    accuracy_alpha = accuracy_score(y_test, y_pred_alpha)
    print(f"Alpha: {alpha}, Accuracy: {accuracy_alpha:.4f}")

"""**Can we visualize the confusion matrix as a heatmap?**"""

# Visualize the confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Confusion Matrix Heatmap")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""**How does the distribution of the target labels vary across different features?**"""

# Visualize the distribution of target labels across different features
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))

for i, ax in enumerate(axes.flatten()):
    if i < len(X.columns):
        sns.boxplot(x=y, y=X[X.columns[i]], ax=ax)
        ax.set_title(f'Distribution of {X.columns[i]} across Target Labels')

plt.tight_layout()
plt.show()

"""**What is the class distribution of the target labels?**"""

# Visualize the class distribution of the target labels
plt.figure(figsize=(8, 6))
sns.countplot(x=y)
plt.title("Class Distribution of Target Labels")
plt.show()

"""**Are there any outliers in the dataset?**"""

# Visualize boxplots to identify outliers
plt.figure(figsize=(15, 12))
sns.boxplot(data=X)
plt.title("Boxplots to Identify Outliers")
plt.xticks(rotation=45)
plt.show()

